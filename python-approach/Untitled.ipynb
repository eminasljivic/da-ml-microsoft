{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "25a88cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import spacy\n",
    "from pytesseract import Output\n",
    "import cv2\n",
    "import json\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.util import filter_spans\n",
    "from spacy.tokens import DocBin\n",
    "from pytesseract import Output\n",
    "import matplotlib.pyplot as plt\n",
    "import camelot.io as camelot\n",
    "from utils.boundingbox import BoundingBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "41cc4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'input/train'\n",
    "test_dir = r'input/test'\n",
    "fields_dir = r'input/fields'\n",
    "\n",
    "testFilePath = train_dir+'/'+os.listdir(train_dir)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82190edc",
   "metadata": {},
   "source": [
    "# Train Named Entity Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9c22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = os.listdir(fields_dir)\n",
    "invoices = os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e14380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(fields))\n",
    "print(len(invoices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4aceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_fields  = set(map(lambda f: f.split('.')[0], fields))\n",
    "set_invoices = set(map(lambda f: f.split('.')[0], invoices))\n",
    "\n",
    "training_set = list(set_fields.intersection(set_invoices))\n",
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8881ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=[\"filename\", \"text\"])\n",
    "\n",
    "data[\"filename\"] = training_set\n",
    "\n",
    "data_text = []\n",
    "for file in data[\"filename\"]:\n",
    "    data_text.append(get_text(f'{train_dir}/{file}.pdf'))\n",
    "data[\"text\"] = data_text\n",
    "\n",
    "ent_list = []\n",
    "for file in data[\"filename\"]:\n",
    "    with open(f'{fields_dir}/{file}.json') as f:\n",
    "        entity_dict = json.load(f)\n",
    "        ent_list.append(entity_dict)\n",
    "data[\"entity_dictionary\"] = ent_list\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb61ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "id_ent = []\n",
    "\n",
    "nlp_match = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp_match.vocab)\n",
    "for index, row in data.iterrows():\n",
    "    ent_dic = row[\"entity_dictionary\"]\n",
    "    ent = []\n",
    "    phrases = list(ent_dic.values())\n",
    "    patterns = [nlp_match.make_doc(phrase) for phrase in phrases]\n",
    "    matcher.add(\"EntityList\", None, *patterns)\n",
    "\n",
    "    doc = nlp_match(row[\"text\"])\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = doc[start:end]\n",
    "            if start>0:\n",
    "                sb = doc[0:start]\n",
    "                start_index=len(sb.text)+1\n",
    "            else:\n",
    "                start_index=0\n",
    "            end_index= start_index+len(span.text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for key, value in ent_dic.items():\n",
    "            if value==span.text:\n",
    "                ent_tup=(start_index, end_index, key)\n",
    "                ent.append(ent_tup)\n",
    "                \n",
    "    ent_set = {\"total\", \"invnr\"}\n",
    "    detected_entities = set([key for start, end, key in ent])\n",
    "    missed_entities = list(ent_set - detected_entities)\n",
    "    if \"total\" in missed_entities:\n",
    "        value = ent_dic[\"total\"]\n",
    "        if len(value)>0:\n",
    "            catch_total = re.search(value, str(row[\"text\"]).replace(\",\", \"\"))\n",
    "            ent_tup = (catch_total.span()[0], catch_total.span()[1], \"total\")\n",
    "            ent.append(ent_tup)\n",
    "    if \"invnr\" in missed_entities:\n",
    "        value = ent_dic[\"invnr\"]\n",
    "        if len(value)>0:\n",
    "            catch_total = re.search(value, str(row[\"text\"]).replace(\",\", \"\"))\n",
    "            ent_tup = (catch_total.span()[0], catch_total.span()[1], \"invnr\")\n",
    "            ent.append(ent_tup)\n",
    "    id_ent.append(len(ent))\n",
    "    training_data.append({\"entities\": ent, \"text\":row[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c351221",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = training_data\n",
    "output_dir=r'model'\n",
    "n_iter = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "doc_bin = DocBin() # create a DocBin object\n",
    "\n",
    "for training_example  in TRAIN_DATA: \n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be347802",
   "metadata": {},
   "source": [
    "# Testing NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = spacy.load(\"model/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(test_dir)\n",
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccaa167",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame(columns=[\"filename\", \"text\"])\n",
    "\n",
    "test_data[\"filename\"] = test_files\n",
    "\n",
    "data_text = []\n",
    "for file in test_data[\"filename\"]:\n",
    "    data_text.append(get_text(f'{test_dir}/{file}'))\n",
    "test_data[\"text\"] = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48050e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "    op_dict = {\"total\": \"\", \"invnr\": \"\"}\n",
    "    doc = nlp_ner(row[\"text\"])\n",
    "    for ent in doc.ents:      #identifying the entities using the trained model\n",
    "        op_dict[ent.label_] = ent.text\n",
    "  \n",
    "    \n",
    "    print(\"Entities\", op_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_old(filename):\n",
    "    doc = convert_from_path(filename)\n",
    "\n",
    "    for page_number, page_data in enumerate(doc):\n",
    "        txt = pytesseract.image_to_string(page_data).replace('\\n', ', ') \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913df493",
   "metadata": {},
   "source": [
    "# Table Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec3eaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image,cmap=None):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(image,cmap=cmap) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1c721f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate two text boxes a larger one that covers them\n",
    "def merge_boxes(box1, box2):\n",
    "    (x1, y1, w1, h1, text1) = box1\n",
    "    (x2, y2, w2, h2, text2) = box2\n",
    "\n",
    "    return [min(x1, x2), \n",
    "         min(y1, y2), \n",
    "         w1 + w2 + calc_horizontal_distance(box1, box2),\n",
    "         max(h1, h2),\n",
    "         text1 + ' ' + text2]\n",
    "\n",
    "\n",
    "def calc_horizontal_distance(box1, box2):\n",
    "    (x1, y1, w1, h1, text1) = box1\n",
    "    (x2, y2, w2, h2, text2) = box2\n",
    "\n",
    "    return abs(min(x1+w1-x2,x2+w2-x1))\n",
    "\n",
    "def get_rows(boxes, cell_threshold=50):\n",
    "    rows = {}\n",
    "\n",
    "    # Clustering the bounding boxes by their positions\n",
    "    for box in boxes:\n",
    "        (x, y, w, h, text) = box\n",
    "        row_key = y // cell_threshold\n",
    "        rows[row_key] = [box] if row_key not in rows else rows[row_key] + [box]\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def merge(boxes, dist_limit = 10, cell_threshold=50):\n",
    "    rows = get_rows(boxes, cell_threshold)\n",
    "    for row in rows.values():\n",
    "        i=0\n",
    "        while i<len(row)-1:\n",
    "            if calc_horizontal_distance(row[i], row[i+1]) <= dist_limit:\n",
    "                row[i] = merge_boxes(row[i], row[i+1])\n",
    "                row.pop(i+1)\n",
    "            else:\n",
    "                i += 1\n",
    "    return [item for sublist in list(rows.values()) for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "868bd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only works if there's only one table on a page\n",
    "# Important parameters:\n",
    "#  - morph_size\n",
    "#  - min_text_height_limit\n",
    "#  - max_text_height_limit\n",
    "#  - cell_threshold\n",
    "#  - min_columns\n",
    "\n",
    "\n",
    "def pre_process_image(img, morph_size=(8, 8)):\n",
    "    pre = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    pre = cv2.threshold(pre, 240, 255, cv2.THRESH_BINARY)[1]\n",
    "    # dilate the text to make it solid spot\n",
    "    cpy = pre.copy()\n",
    "    struct = cv2.getStructuringElement(cv2.MORPH_RECT, morph_size)\n",
    "    cpy = cv2.dilate(~cpy, struct, anchor=(-1, -1), iterations=1)\n",
    "    pre = ~cpy\n",
    "    return pre\n",
    "\n",
    "\n",
    "def find_text_boxes(pre, min_text_height_limit=6, max_text_height_limit=40):\n",
    "    boxes = []\n",
    "\n",
    "    d = pytesseract.image_to_data(pre, output_type=Output.DICT)\n",
    "    for i in range(len(d['level'])):\n",
    "        (x, y, w, h, text) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i], d['text'][i])\n",
    "\n",
    "        if min_text_height_limit < h < max_text_height_limit and d['text'][i].strip():\n",
    "            boxes.append([x, y, w, h, text])\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def find_table_in_boxes(boxes, cell_threshold=10, min_columns=2):\n",
    "    rows = {}\n",
    "    cols = {}\n",
    "\n",
    "    # Clustering the bounding boxes by their positions\n",
    "    for box in boxes:\n",
    "        (x, y, w, h, text) = box\n",
    "        col_key = x // cell_threshold\n",
    "        row_key = y // cell_threshold\n",
    "        cols[col_key] = [box] if col_key not in cols else cols[col_key] + [box]\n",
    "        rows[row_key] = [box] if row_key not in rows else rows[row_key] + [box]\n",
    "\n",
    "    # Filtering out the clusters having less than 2 cols\n",
    "    table_cells = list(filter(lambda r: len(r) >= min_columns, rows.values()))\n",
    "    # Sorting the row cells by x coord\n",
    "    table_cells = [list(sorted(tb)) for tb in table_cells]\n",
    "    # Sorting rows by the y coord\n",
    "    table_cells = list(sorted(table_cells, key=lambda r: r[0][1]))\n",
    "\n",
    "    return table_cells\n",
    "\n",
    "\n",
    "def build_lines(table_cells):\n",
    "    if table_cells is None or len(table_cells) <= 0:\n",
    "        return [], []\n",
    "\n",
    "    max_last_col_width_row = max(table_cells, key=lambda b: b[-1][2])\n",
    "    max_x = max_last_col_width_row[-1][0] + max_last_col_width_row[-1][2]\n",
    "\n",
    "    max_last_row_height_box = max(table_cells[-1], key=lambda b: b[3])\n",
    "    max_y = max_last_row_height_box[1] + max_last_row_height_box[3]\n",
    "\n",
    "    hor_lines = []\n",
    "    ver_lines = []\n",
    "\n",
    "    for box in table_cells:\n",
    "        x = box[0][0]\n",
    "        y = box[0][1]\n",
    "        hor_lines.append((x, y, max_x, y))\n",
    "\n",
    "    for box in table_cells[0]:\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        ver_lines.append((x, y, x, max_y))\n",
    "\n",
    "    (x, y, w, h, text) = table_cells[0][-1]\n",
    "    ver_lines.append((max_x, y, max_x, max_y))\n",
    "    (x, y, w, h, text) = table_cells[0][0]\n",
    "    hor_lines.append((x, max_y, max_x, max_y))\n",
    "\n",
    "    return hor_lines, ver_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4a5d33a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BoundingBoxes.get_bounding_boxes_from_img() missing 1 required positional argument: 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z3/721ychmx5zlchxrfz865dc2c0000gn/T/ipykernel_12332/622547777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpre_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoundingBoxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bounding_boxes_from_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtable_cells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_table_in_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhor_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mver_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_cells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: BoundingBoxes.get_bounding_boxes_from_img() missing 1 required positional argument: 'img'"
     ]
    }
   ],
   "source": [
    "pages = convert_from_path(train_dir+'/'+os.listdir(train_dir)[12])\n",
    "img = np.array(pages[0])\n",
    "\n",
    "pre_processed = pre_process_image(img, morph_size=(1,1))\n",
    "rows = BoundingBoxes.get_bounding_boxes_from_img(pre_processed)\n",
    "table_cells = find_table_in_boxes(rows, min_columns=4, cell_threshold=25)\n",
    "hor_lines, ver_lines = build_lines(table_cells)\n",
    "\n",
    "# Visualize the result\n",
    "vis = img.copy()\n",
    "\n",
    "for cell in table_cells:\n",
    "    for c in cell:\n",
    "        (x, y, w, h, text) = c\n",
    "        cv2.rectangle(vis, (x, y), (x + w - 2, y + h - 2), (255, 0, 0), 3)\n",
    "\n",
    "for box in rows:\n",
    "    (x, y, w, h, text) = box\n",
    "    cv2.rectangle(vis, (x, y), (x + w - 2, y + h - 2), (0, 255, 0), 1)\n",
    "\n",
    "for line in hor_lines:\n",
    "    [x1, y1, x2, y2] = line\n",
    "    cv2.line(vis, (x1, y1), (x2, y2), (0, 0, 255), 1)\n",
    "\n",
    "for line in ver_lines:\n",
    "    [x1, y1, x2, y2] = line\n",
    "    cv2.line(vis, (x1, y1), (x2, y2), (0, 0, 255), 1)\n",
    "\n",
    "plot(vis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38449c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('leoenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d062e205adac94e46c48383f689092959cf26a5fbf115ddfc7417b9fa8d442d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
